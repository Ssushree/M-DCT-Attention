{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828c4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from tensorflow.keras.engine.topology import Layer\n",
    "#from tensorflow.keras.utils.data_utils import get_file\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy.random as random\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import r_\n",
    "#from keras.utils.training_utils import multi_gpu_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b87a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_dct [[ 1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1. -1. -1. -1. -1.]\n",
      " [ 1.  1. -1. -1. -1. -1.  1.  1.]\n",
      " [ 1. -1. -1. -1.  1.  1.  1. -1.]\n",
      " [ 1. -1. -1.  1.  1. -1. -1.  1.]\n",
      " [ 1. -1.  1.  1. -1. -1.  1. -1.]\n",
      " [ 1. -1.  1. -1. -1.  1. -1.  1.]\n",
      " [ 1. -1.  1. -1.  1. -1.  1. -1.]] (8, 8)\n",
      "s_dct [[0.35355339 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.35355339 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.35355339 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.35355339]] (8, 8)\n",
      "c_dct [[ 0.35355339  0.35355339  0.35355339  0.35355339  0.35355339  0.35355339\n",
      "   0.35355339  0.35355339]\n",
      " [ 0.35355339  0.35355339  0.35355339  0.35355339 -0.35355339 -0.35355339\n",
      "  -0.35355339 -0.35355339]\n",
      " [ 0.35355339  0.35355339 -0.35355339 -0.35355339 -0.35355339 -0.35355339\n",
      "   0.35355339  0.35355339]\n",
      " [ 0.35355339 -0.35355339 -0.35355339 -0.35355339  0.35355339  0.35355339\n",
      "   0.35355339 -0.35355339]\n",
      " [ 0.35355339 -0.35355339 -0.35355339  0.35355339  0.35355339 -0.35355339\n",
      "  -0.35355339  0.35355339]\n",
      " [ 0.35355339 -0.35355339  0.35355339  0.35355339 -0.35355339 -0.35355339\n",
      "   0.35355339 -0.35355339]\n",
      " [ 0.35355339 -0.35355339  0.35355339 -0.35355339 -0.35355339  0.35355339\n",
      "  -0.35355339  0.35355339]\n",
      " [ 0.35355339 -0.35355339  0.35355339 -0.35355339  0.35355339 -0.35355339\n",
      "   0.35355339 -0.35355339]] (8, 8)\n",
      "c_dct_inv [[ 0.35355339  0.70710678  0.35355339  0.70710678  0.35355339  0.\n",
      "   0.35355339  0.        ]\n",
      " [ 0.35355339  0.70710678  0.35355339  0.         -0.35355339 -0.70710678\n",
      "  -0.35355339  0.        ]\n",
      " [ 0.35355339  0.         -0.35355339 -0.70710678 -0.35355339  0.\n",
      "   0.35355339  0.70710678]\n",
      " [ 0.35355339  0.         -0.35355339  0.          0.35355339  0.70710678\n",
      "  -0.35355339 -0.70710678]\n",
      " [ 0.35355339  0.         -0.35355339  0.          0.35355339 -0.70710678\n",
      "  -0.35355339  0.70710678]\n",
      " [ 0.35355339  0.         -0.35355339  0.70710678 -0.35355339  0.\n",
      "   0.35355339 -0.70710678]\n",
      " [ 0.35355339 -0.70710678  0.35355339  0.         -0.35355339  0.70710678\n",
      "  -0.35355339  0.        ]\n",
      " [ 0.35355339 -0.70710678  0.35355339 -0.70710678  0.35355339  0.\n",
      "   0.35355339  0.        ]] (8, 8)\n"
     ]
    }
   ],
   "source": [
    "def n_mode_product(x, u, n):\n",
    "    n = int(n)\n",
    "    if n > 26:\n",
    "        raise ValueError('n is too large.')\n",
    "    ind = ''.join(chr(ord('a') + i) for i in range(n))\n",
    "\n",
    "    return tf.einsum(f'L{ind}K...,JK->L{ind}J...', x, u)\n",
    "\n",
    "t_dct = np.array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                  [1, 1, 1, 1, -1, -1, -1, -1],\n",
    "                  [1, 1, -1, -1, -1, -1, 1, 1],\n",
    "                  [1, -1, -1, -1, 1, 1, 1, -1],\n",
    "                  [1, -1, -1, 1, 1, -1, -1, 1],\n",
    "                  [1, -1, 1, 1, -1, -1, 1, -1],\n",
    "                  [1, -1, 1, -1, -1, 1, -1, 1],\n",
    "                  [1, -1, 1, -1, 1, -1, 1, -1]],  dtype = 'float')\n",
    "print('t_dct', t_dct, t_dct.shape)\n",
    "s_dct = np.diag([1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8), 1/np.sqrt(8)])\n",
    "print('s_dct', s_dct, s_dct.shape)\n",
    "\n",
    "c_dct = np.matmul(s_dct, t_dct)\n",
    "if (np.matmul(np.transpose(c_dct), c_dct)==np.identity(8)).any():\n",
    "    c_dct_inv = np.matmul(np.transpose(t_dct), s_dct)\n",
    "else:\n",
    "    c_dct_inv = np.matmul(np.linalg.inv(t_dct), np.linalg.inv(s_dct))\n",
    "\n",
    "print('c_dct', c_dct, c_dct.shape)\n",
    "print('c_dct_inv',c_dct_inv, c_dct_inv.shape)\n",
    "\n",
    "def cal_tensor_dct_coeff(inp):\n",
    "    bs, w, h, c = inp.shape\n",
    "    \n",
    "    dct_coeff = K.ones_like(K.variable(np.random.random((bs, w, h, c))))\n",
    "    dct_coeff = tf.Variable(dct_coeff)\n",
    "    block_size = 8\n",
    "    #for b in range(bs):\n",
    "    for i in r_[:w:block_size]:\n",
    "        for j in r_[:h:block_size]:\n",
    "            for k in r_[:c:block_size]:\n",
    "                coe = Lambda(lambda x: n_mode_product(n_mode_product(n_mode_product(n_mode_product(n_mode_product(n_mode_product(x[0], x[1], 0), x[1], 1), x[1], 2), x[2], 0), x[2], 1), x[2], 2))([inp[:, i:(i+block_size), j:(j+block_size), k:(k+block_size)], t_dct, s_dct])\n",
    "                dct_coeff[:, i:(i+block_size), j:(j+block_size), k:(k+block_size)].assign(coe)\n",
    "    dct_coeff = tf.convert_to_tensor(dct_coeff)\n",
    "    return dct_coeff\n",
    "\n",
    "    \n",
    "def cal_thresholded_coeff(dct_coeff, energy_retained):\n",
    "    bs, w, h, c = dct_coeff.shape\n",
    "    \n",
    "    dct_coeff_thresh_selected = K.zeros_like(K.variable(np.random.random((bs, w, h, c))))\n",
    "    dct_coeff_thresh_selected = tf.Variable(dct_coeff_thresh_selected)\n",
    "    percent_energy_thresholded = K.zeros_like(K.variable(np.random.random((bs))))\n",
    "    percent_energy_thresholded = tf.Variable(percent_energy_thresholded)\n",
    "    \n",
    "    thresh_range = [0.8, 0.6, 0.4, 0.2, 0.1, 0.08, 0.06, 0.04, 0.02, 0.01, 0.008, 0.006, 0.004, 0.002, 0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "    \n",
    "    for b in range(bs):\n",
    "        for th in range(len(thresh_range)):\n",
    "            thresh = thresh_range[th]\n",
    "            coeff_thresh = tf.math.multiply(dct_coeff[b], (tf.cast(tf.math.abs(dct_coeff[b]) > tf.math.multiply(thresh, tf.math.reduce_max(dct_coeff[b], axis = [1, 2], keepdims = True)), tf.float32)))\n",
    "            percent_energy = (tf.reduce_sum(tf.math.square(coeff_thresh))/ tf.reduce_sum(tf.math.square(dct_coeff[b])))*100\n",
    "            percent_energy_thresholded[b].assign(percent_energy)\n",
    "            if percent_energy_thresholded[b] >= energy_retained:\n",
    "                dct_coeff_thresh_selected[b].assign(coeff_thresh)\n",
    "                break\n",
    "    dct_coeff_thresh_selected = tf.convert_to_tensor(dct_coeff_thresh_selected)\n",
    "    percent_energy_thresholded = tf.convert_to_tensor(percent_energy_thresholded)\n",
    "    #return dct_coeff_thresh_selected, percent_energy_thresholded\n",
    "    return dct_coeff_thresh_selected\n",
    "        \n",
    "def cal_reconstructed_feat(dct_coeff_thresh_selected):\n",
    "    bs, w, h, c = dct_coeff_thresh_selected.shape    \n",
    "    recon_feat_vis_threshold = K.zeros_like(K.variable(np.random.random((bs, w, h, c))))\n",
    "    recon_feat_vis_threshold = tf.Variable(recon_feat_vis_threshold)\n",
    "    block_size = 8\n",
    "    #for b in range(bs):\n",
    "    for i in r_[:w:block_size]:\n",
    "        for j in r_[:h:block_size]:\n",
    "            for k in r_[:c:block_size]:\n",
    "                recon = Lambda(lambda x: n_mode_product(n_mode_product(n_mode_product(x[0], x[1], 0), x[1], 1), x[1], 2))([dct_coeff_thresh_selected[:, i:(i+block_size), j:(j+block_size), k:(k+block_size)], c_dct_inv])\n",
    "                recon_feat_vis_threshold[:, i:(i+block_size), j:(j+block_size), k:(k+block_size)].assign(recon)\n",
    "    recon_feat_vis_threshold = tf.convert_to_tensor(recon_feat_vis_threshold)\n",
    "    return recon_feat_vis_threshold \n",
    "\n",
    "def tensor_dct(inp, thresh):\n",
    "    dct_coeff = cal_tensor_dct_coeff(inp)\n",
    "    dct_coeff_thresh_selected = cal_thresholded_coeff(dct_coeff, thresh)\n",
    "    recon_feat_vis_threshold = cal_reconstructed_feat(dct_coeff_thresh_selected)\n",
    "    return recon_feat_vis_threshold\n",
    "    \n",
    "def att_dct(inp, thresh):\n",
    "    recon_feat_vis_threshold = tensor_dct(inp, thresh)\n",
    "    error_score = Lambda(lambda x: 1 - K.softmax(tf.subtract(x[0], x[1])))([inp, recon_feat_vis_threshold])\n",
    "    att_output = Lambda(lambda x: tf.math.multiply(x[0], tf.math.multiply(x[0], x[1])))([inp, error_score])\n",
    "    return att_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214166e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_25 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_26 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_27 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_29 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_30 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_31 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_33 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_34 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_35 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_37 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_38 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_39 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_40 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_42 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_43 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_44 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_46 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_47 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_50 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_52 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_54 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_55 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_58 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_59 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_60 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_61 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_62 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_63 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_66 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_67 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_70 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_71 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_73 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_74 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_75 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_76 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_78 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_79 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_80 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_81 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_82 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_83 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_84 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_85 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_86 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_87 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_88 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_89 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_90 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_91 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_92 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_93 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_94 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_95 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_96 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_97 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_98 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_99 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_100 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_101 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_103 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_104 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_105 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_106 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_107 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_108 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_109 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_111 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_112 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_113 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_115 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_116 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_117 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_118 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_119 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_120 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_121 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_122 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_123 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_124 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_125 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_126 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_127 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_128 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_129 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_133 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_134 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_135 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_137 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_138 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_139 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_141 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_142 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_143 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_146 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_147 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_149 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_151 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_153 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_154 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_155 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_157 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_158 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_159 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_161 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_162 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_163 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_166 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_167 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_169 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_170 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_171 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_172 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_173 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_175 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_177 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_178 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_179 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_181 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_182 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_183 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_185 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_186 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_187 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_189 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_190 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_191 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_193 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_194 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_195 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lambda_197 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(32, 128, 128, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(32, 128, 128, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (32, 128, 128, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (32, 128, 128, 16)   448         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (32, 128, 128, 16)   2320        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (32, 128, 128, 16)   2320        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (32, 64, 64, 16)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (32, 64, 64, 16)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (32, 64, 64, 32)     4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (32, 64, 64, 32)     4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (32, 64, 64, 32)     9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (32, 64, 64, 32)     9248        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (32, 32, 32, 32)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (32, 32, 32, 32)     0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (32, 32, 32, 64)     18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (32, 32, 32, 64)     18496       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (32, 32, 32, 64)     36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (32, 32, 32, 64)     36928       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (32, 16, 16, 64)     0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (32, 16, 16, 64)     0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub (TensorFlowOpLa [(32, 16, 16, 64)]   0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_2 (TensorFlowOp [(32, 16, 16, 64)]   0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorFlow [(32, 16, 16, 64)]   0           tf_op_layer_Sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_1 (TensorFl [(32, 16, 16, 64)]   0           tf_op_layer_Sub_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlowOp [(32, 16, 16, 64)]   0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_3 (TensorFlowOp [(32, 16, 16, 64)]   0           tf_op_layer_Softmax_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_65 (Lambda)              (32, 16, 16, 64)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 tf_op_layer_Sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_131 (Lambda)             (32, 16, 16, 64)     0           max_pooling2d_5[0][0]            \n",
      "                                                                 tf_op_layer_Sub_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (32, 16, 16, 64)     256         lambda_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (32, 16, 16, 64)     256         lambda_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (32, 16, 16, 128)    73856       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (32, 16, 16, 128)    73856       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (32, 16, 16, 128)    147584      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (32, 16, 16, 128)    147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (32, 8, 8, 128)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (32, 8, 8, 128)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_4 (TensorFlowOp [(32, 8, 8, 128)]    0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_6 (TensorFlowOp [(32, 8, 8, 128)]    0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_2 (TensorFl [(32, 8, 8, 128)]    0           tf_op_layer_Sub_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_3 (TensorFl [(32, 8, 8, 128)]    0           tf_op_layer_Sub_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_5 (TensorFlowOp [(32, 8, 8, 128)]    0           tf_op_layer_Softmax_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sub_7 (TensorFlowOp [(32, 8, 8, 128)]    0           tf_op_layer_Softmax_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_165 (Lambda)             (32, 8, 8, 128)      0           max_pooling2d_6[0][0]            \n",
      "                                                                 tf_op_layer_Sub_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_199 (Lambda)             (32, 8, 8, 128)      0           max_pooling2d_7[0][0]            \n",
      "                                                                 tf_op_layer_Sub_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (32, 8, 8, 128)      512         lambda_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (32, 8, 8, 128)      512         lambda_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (32, 8192)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (32, 8192)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (32, 8192)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (32, 8192)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (32, 512)            4194816     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (32, 512)            4194816     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (32, 512)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (32, 512)            0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (32, 512)            262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (32, 512)            262656      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_200 (Lambda)             (32, 512)            0           dense_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (32, 2)              1026        lambda_200[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 9,504,546\n",
      "Trainable params: 9,503,778\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "width, height, num_ch = 128, 128, 3\n",
    "num_classes = 418\n",
    "\n",
    "def branch_model(batch_size, width, height, num_ch, num_classes, learning_rate, d, thresh):\n",
    "    imgv = Input(batch_shape=(batch_size, width, height, num_ch))\n",
    "    imgn = Input(batch_shape=(batch_size, width, height, num_ch))\n",
    "\n",
    "    ## Block 1\n",
    "    conv1_1v = Conv2D(16, (3, 3), activation=\"relu\", padding = 'same')(imgv)\n",
    "    conv1_2v = Conv2D(16, (3, 3), activation=\"relu\", padding = 'same')(conv1_1v) \n",
    "\n",
    "    pool1v = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2v)# 48, 48, 16\n",
    "\n",
    "    conv1_1n = Conv2D(16, (3, 3), activation=\"relu\", padding = 'same')(imgn)\n",
    "    conv1_2n = Conv2D(16, (3, 3), activation=\"relu\", padding = 'same')(conv1_1n) \n",
    "\n",
    "    pool1n = MaxPooling2D((2, 2), strides=(2, 2))(conv1_2n)# 48, 48, 16\n",
    "    \n",
    "    ## Block 2\n",
    "    conv2_1v = Conv2D(32, (3, 3), activation=\"relu\", padding = 'same')(pool1v)\n",
    "    conv2_2v = Conv2D(32, (3, 3), activation=\"relu\", padding = 'same')(conv2_1v)\n",
    "    \n",
    "    pool2v = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2v)# 24, 24, 32\n",
    "    #attended_pool2v = att_dct(pool2v, thresh)\n",
    "    #att_out_pool2v = BatchNormalization()(attended_pool2v)\n",
    "    \n",
    "    conv2_1n = Conv2D(32, (3, 3), activation=\"relu\", padding = 'same')(pool1n)\n",
    "    conv2_2n = Conv2D(32, (3, 3), activation=\"relu\", padding = 'same')(conv2_1n)\n",
    "    \n",
    "    pool2n = MaxPooling2D((2, 2), strides=(2, 2))(conv2_2n)# 24, 24, 32\n",
    "    #attended_pool2n = att_dct(pool2n, thresh)\n",
    "    #att_out_pool2n = BatchNormalization()(attended_pool2n)\n",
    "    \n",
    "    ## Block 3\n",
    "    conv3_1v = Conv2D(64, (3, 3), activation=\"relu\", padding = 'same')(pool2v)\n",
    "    conv3_2v = Conv2D(64, (3, 3), activation=\"relu\", padding = 'same')(conv3_1v)\n",
    "\n",
    "    pool3v = MaxPooling2D((2, 2), strides=(2, 2))(conv3_2v)# 12, 12, 64\n",
    "    attended_pool3v = att_dct(pool3v, thresh)\n",
    "    att_out_pool3v = BatchNormalization()(attended_pool3v)\n",
    "    \n",
    "    conv3_1n = Conv2D(64, (3, 3), activation=\"relu\", padding = 'same')(pool2n)\n",
    "    conv3_2n = Conv2D(64, (3, 3), activation=\"relu\", padding = 'same')(conv3_1n)\n",
    "\n",
    "    pool3n = MaxPooling2D((2, 2), strides=(2, 2))(conv3_2n)# 12, 12, 64\n",
    "    attended_pool3n = att_dct(pool3n, thresh)\n",
    "    att_out_pool3n = BatchNormalization()(attended_pool3n)\n",
    "    \n",
    "    ## Block 4\n",
    "    conv4_1v = Conv2D(128, (3, 3), activation=\"relu\", padding = 'same')(att_out_pool3v)\n",
    "    conv4_2v = Conv2D(128, (3, 3), activation=\"relu\", padding = 'same')(conv4_1v)\n",
    "\n",
    "    pool4v = MaxPooling2D((2, 2), strides=(2, 2))(conv4_2v)# 6, 6, 128\n",
    "    attended_pool4v = att_dct(pool4v, thresh)\n",
    "    att_out_pool4v = BatchNormalization()(attended_pool4v)\n",
    "    \n",
    "    conv4_1n = Conv2D(128, (3, 3), activation=\"relu\", padding = 'same')(att_out_pool3n)\n",
    "    conv4_2n = Conv2D(128, (3, 3), activation=\"relu\", padding = 'same')(conv4_1n)\n",
    "\n",
    "    pool4n = MaxPooling2D((2, 2), strides=(2, 2))(conv4_2n)# 6, 6, 128\n",
    "    attended_pool4n = att_dct(pool4n, thresh)\n",
    "    att_out_pool4n = BatchNormalization()(attended_pool4n)\n",
    "    \n",
    "    flatten_features_5v = Flatten()(att_out_pool4v)\n",
    "    drop1_features_5v = Dropout(d)(flatten_features_5v)\n",
    "    dense1_features_5v = Dense(512, activation='relu')(drop1_features_5v)\n",
    "    drop2_features_5v = Dropout(d)(dense1_features_5v)\n",
    "    dense2_features_5v = Dense(512, activation='relu')(drop2_features_5v)\n",
    "\n",
    "    flatten_features_5n = Flatten()(att_out_pool4n)\n",
    "    drop1_features_5n = Dropout(d)(flatten_features_5n)\n",
    "    dense1_features_5n= Dense(512, activation='relu')(drop1_features_5n)\n",
    "    drop2_features_5n = Dropout(d)(dense1_features_5n)\n",
    "    dense2_features_5n = Dense(512, activation='relu')(drop2_features_5n)\n",
    "    \n",
    "    L2_layer = Lambda(lambda tensors: K.square(tensors[0] - tensors[1]))\n",
    "    distance = L2_layer([dense2_features_5v, dense2_features_5n])\n",
    "    prediction = Dense(2, activation='softmax')(distance)\n",
    "\n",
    "    siamese_net = Model(inputs=[imgv, imgn], outputs=prediction)\n",
    "    #sgd = SGD(lr=learning_rate, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "    #siamese_net.compile(loss=\"categorical_crossentropy\", metrics = ['accuracy'], optimizer = sgd)\n",
    "    siamese_net.compile(loss=\"categorical_crossentropy\", metrics = ['accuracy'], optimizer=Adam(learning_rate = 0.0001))\n",
    "    siamese_net.summary()\n",
    "    \n",
    "    return siamese_net\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.2\n",
    "energy_retained = 99.0\n",
    "siamese_net = branch_model(batch_size, width, height, num_ch, num_classes, learning_rate, dropout, energy_retained)\n",
    "print(len(siamese_net.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c62729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:/Sushree/Tensor_DCT/Results/Identification/polyu/'\n",
    "name = 'weights_siamese_net_dct_energy99.0_maxpool3and4_polyu_adam_bch32_lr10-4_drp0.2_200eph'\n",
    "\n",
    "siamese_net.load_weights(save_path + name + \".h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bef615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imlist(path, option):\n",
    "    if option == 'tiff':\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.tiff')] # PolyU\n",
    "    elif option == 'bmp':\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.bmp')] # Cross-eyed\n",
    "    elif option == 'jpg':\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg')] # IMP\n",
    "    elif option == 'png':\n",
    "        return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.png')] # Segmentation map\n",
    "\n",
    "# The function 'create_data_label' is to create the database by stacking raw images onto one another along with corresponding labels.\n",
    "def create_data_label(path, width, height, option):\n",
    "    folder_list = os.listdir(path)\n",
    "    num_class = len(folder_list)\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(len(folder_list)):\n",
    "        img_list = get_imlist(os.path.join(path, folder_list[i]), option)\n",
    "        for j in range(len(img_list)):\n",
    "            img = image.load_img(img_list[j], target_size=(width, height))\n",
    "            img = np.array(img)\n",
    "            x = image.img_to_array(img)\n",
    "            data.append(x)\n",
    "            label.append(i)\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    return data, label\n",
    "\n",
    "def get_index(n_classes, n_images):\n",
    "    num_genuines =  (n_images**2) * n_classes # 10000\n",
    "    total_num_imposters = (n_images**2) * n_classes * (n_classes - 1) # 990000\n",
    "\n",
    "    genuine_index_pair = np.zeros((num_genuines, 2))\n",
    "    imposter_index_pair = np.zeros((total_num_imposters, 2))\n",
    "    genuine_folder_pair = np.zeros((num_genuines, 2))\n",
    "    imposter_folder_pair = np.zeros((total_num_imposters, 2))\n",
    "    genuine_count = 0\n",
    "    imposter_count = 0\n",
    "    for i_1 in range (n_classes): # 100 (0 to 99 )\n",
    "        for i_2 in range(n_classes): # 100\n",
    "            if i_1 == i_2:\n",
    "                index_1 = list(range((i_1*n_images), ((i_1+1)*n_images) ))\n",
    "                index_2 = list(range((i_2*n_images), ((i_2+1)*n_images) ))\n",
    "                for j_1 in range(len(index_1)): # 0 to 9\n",
    "                    for j_2 in range(len(index_2)): # 0 to 9\n",
    "                        index_pair = np.transpose([index_1[j_1], index_2[j_2]])\n",
    "                        folder_pair = np.transpose([i_1, i_2])\n",
    "                        genuine_index_pair[genuine_count] = index_pair\n",
    "                        genuine_folder_pair[genuine_count] = folder_pair\n",
    "                        genuine_count = genuine_count + 1\n",
    "            else:\n",
    "                index_1 = list(range((i_1*n_images), ((i_1+1)*n_images) ))\n",
    "                index_2 = list(range((i_2*n_images), ((i_2+1)*n_images) ))\n",
    "                for j_1 in range(len(index_1)): # 0 to 9\n",
    "                    for j_2 in range(len(index_2)): # 0 to 9\n",
    "                        index_pair = np.transpose([index_1[j_1], index_2[j_2]])\n",
    "                        folder_pair = np.transpose([i_1, i_2])\n",
    "                        imposter_index_pair[imposter_count] = index_pair\n",
    "                        imposter_folder_pair[imposter_count] = folder_pair\n",
    "                        imposter_count = imposter_count + 1\n",
    "    return genuine_index_pair, genuine_folder_pair, genuine_count, imposter_index_pair, imposter_folder_pair, imposter_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbde7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Training Data\n",
    "train_vis_path = 'C:/Sushree/Datasets/PolyU/Train/VIS'\n",
    "train_vis_data, train_vis_label = create_data_label(train_vis_path, width, height, 'tiff')\n",
    "print(train_vis_data.shape)\n",
    "print('length of train_vis_label', len(train_vis_label))\n",
    "\n",
    "train_nir_path = 'C:/Sushree/Datasets/PolyU/Train/NIR'\n",
    "train_nir_data, train_nir_label = create_data_label(train_nir_path, width, height, 'tiff')\n",
    "print(train_nir_data.shape)\n",
    "print('length of train_nir_label', len(train_nir_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b88b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_training_images_per_class = 9\n",
    "num_training_images = num_classes* num_training_images_per_class\n",
    "\n",
    "genuine_index_pair, genuine_folder_pair, genuine_count, imposter_index_pair, imposter_folder_pair, imposter_count = get_index(num_classes, num_training_images_per_class)\n",
    "\n",
    "n_genuine = list(range(0, genuine_count))\n",
    "n_imposter = list(range(0, imposter_count))\n",
    "\n",
    "train_genuine_index_pair, val_genuine_index_pair, train_n_genuine, val_n_genuine = train_test_split(genuine_index_pair, n_genuine, test_size=0.33, random_state=42)\n",
    "train_imposter_index_pair, val_imposter_index_pair, train_n_imposter, val_n_imposter = train_test_split(imposter_index_pair, n_imposter, test_size=0.33, random_state=42)\n",
    "\n",
    "train_genuine_count = len(train_n_genuine)\n",
    "train_imposter_count = len(train_n_imposter)\n",
    "true_train_imposter_count = train_genuine_count\n",
    "val_genuine_count = len(val_n_genuine)\n",
    "val_imposter_count = len(val_n_imposter)\n",
    "true_val_imposter_count = val_genuine_count\n",
    "\n",
    "print(train_genuine_count)\n",
    "print(train_imposter_count)\n",
    "print(true_train_imposter_count)\n",
    "print(val_genuine_count )\n",
    "print(val_imposter_count)\n",
    "print(true_val_imposter_count)\n",
    "\n",
    "print('batch_size',batch_size)\n",
    "def my_data_generator_train(batch_size, width, height, num_ch, train_vis_data, train_nir_data, genuine_index_pair, genuine_count, imposter_index_pair, imposter_count, true_imposter_count):\n",
    "    c = 0\n",
    "    true_imposter_index = random.randint(imposter_count, size = true_imposter_count)\n",
    "    total_count = genuine_count + true_imposter_count\n",
    "    n_total = list(range(0, total_count))\n",
    "    \n",
    "    true_imposter_index_pair = np.zeros((true_imposter_count, 2))\n",
    "    for i in range(true_imposter_count):\n",
    "        true_imposter_index_pair[i] = imposter_index_pair[true_imposter_index[i]]\n",
    "        \n",
    "    total_index_pair = np.concatenate((genuine_index_pair, true_imposter_index_pair), axis = 0)\n",
    "    total_targets = np.concatenate((np.ones((genuine_count, )),  np.zeros((true_imposter_count, ))), axis = 0) \n",
    "    total_targets = to_categorical(total_targets, 2)\n",
    "    random.shuffle(n_total) \n",
    "    while(True):\n",
    "        img_vis = np.zeros((batch_size, width, height, num_ch))\n",
    "        img_nir = np.zeros((batch_size, width, height, num_ch))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "        if (c+batch_size < total_count):\n",
    "            for i in range(c,c+batch_size):     \n",
    "                img_vis[i-c]=train_vis_data[int(total_index_pair[n_total[i]][0])].reshape(width, height, num_ch)\n",
    "\n",
    "                img_nir[i-c]=train_nir_data[int(total_index_pair[n_total[i]][1])].reshape(width, height, num_ch)\n",
    "\n",
    "                targets[i-c] = total_targets[n_total[i]]\n",
    "            input_pairs = [img_vis, img_nir]\n",
    "            c+=batch_size\n",
    "        if (c+batch_size >= total_count):\n",
    "            c = 0\n",
    "        yield input_pairs, targets\n",
    "\n",
    "train_gen = my_data_generator_train(batch_size, width, height, num_ch, train_vis_data, train_nir_data, train_genuine_index_pair, train_genuine_count, train_imposter_index_pair, train_imposter_count, true_train_imposter_count)\n",
    "val_gen = my_data_generator_train(batch_size, width, height, num_ch, train_vis_data, train_nir_data, val_genuine_index_pair, val_genuine_count, val_imposter_index_pair, val_imposter_count, true_val_imposter_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa466e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# TRAINING and VALIDATION\n",
    "save_path = 'C:/Sushree/Tensor_DCT/Results/'\n",
    "name = 'siamese_net_dct_energy99.99_maxpool3and4_polyu_adam_bch32_lr10-4_drp0.2_200eph'\n",
    "\n",
    "def get_callbacks(patience_lr):\n",
    "            #mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "            reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n",
    "            return [reduce_lr_loss]\n",
    "        \n",
    "#name_weights = save_path + \"best_weights_\" + name + \".h5\"\n",
    "\n",
    "callbacks = get_callbacks(patience_lr=10)\n",
    "\n",
    "history = siamese_net.fit(train_gen, steps_per_epoch = int(2*train_genuine_count/batch_size), epochs = 200, verbose = 1, validation_data = val_gen, validation_steps = int(2*val_genuine_count/batch_size), callbacks = callbacks)\n",
    "#history = siamese_net.fit(train_gen, steps_per_epoch = int(2*train_genuine_count/batch_size), epochs = 200, verbose = 1, callbacks = callbacks)\n",
    "\n",
    "siamese_net.save_weights(save_path + \"weights_\" + name + \".h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42210f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(save_path + 'acc_' + name + '.png')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(save_path + 'loss_' + name + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8825d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15048\n",
      "6275016\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "test_vis_path = 'C:/Sushree/Datasets/PolyU/Test/VIS' \n",
    "test_vis_data, test_vis_label = create_data_label(test_vis_path, width, height, 'tiff')\n",
    "\n",
    "test_nir_path = 'C:/Sushree/Datasets/PolyU/Test/NIR'\n",
    "test_nir_data, test_nir_label = create_data_label(test_nir_path, width, height, 'tiff')\n",
    "\n",
    "num_testing_images_per_class = 6\n",
    "num_testing_images = num_classes* num_testing_images_per_class\n",
    "\n",
    "test_genuine_index_pair, test_genuine_folder_pair, test_genuine_count, test_imposter_index_pair, test_imposter_folder_pair, test_imposter_count = get_index(num_classes, num_testing_images_per_class)\n",
    "print(test_genuine_count)\n",
    "print(test_imposter_count)\n",
    "\n",
    "def my_data_generator_test(batch_size, width, height, num_ch, train_vis_data, train_nir_data, genuine_index_pair, genuine_count, imposter_index_pair, imposter_count, option):\n",
    "    c = 0\n",
    "    total_count = genuine_count + imposter_count\n",
    "    n_total = list(range(0, total_count))\n",
    "    total_index_pair = np.concatenate((genuine_index_pair, imposter_index_pair), axis = 0)\n",
    "    total_targets = np.concatenate((np.ones((genuine_count, )),  np.zeros((imposter_count, ))), axis = 0)\n",
    "    total_targets = to_categorical(total_targets, 2)\n",
    "\n",
    "    while(True):\n",
    "        img_vis = np.zeros((batch_size, width, height, num_ch))\n",
    "        img_nir = np.zeros((batch_size, width, height, num_ch))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "        if (c+batch_size < total_count):\n",
    "            for i in range(c,c+batch_size):     \n",
    "                img_vis[i-c]=train_vis_data[int(total_index_pair[n_total[i]][0])].reshape(width, height, num_ch)\n",
    "\n",
    "                img_nir[i-c]=train_nir_data[int(total_index_pair[n_total[i]][1])].reshape(width, height, num_ch)\n",
    "\n",
    "                targets[i-c] = total_targets[n_total[i]]\n",
    "            input_pairs = [img_vis, img_nir]\n",
    "            c+=batch_size\n",
    "        if (c+batch_size >= total_count):\n",
    "            c = 0\n",
    "        if option == 'evaluate':    \n",
    "            yield input_pairs, targets\n",
    "        elif option == 'predict':\n",
    "            yield input_pairs\n",
    "              \n",
    "test_gen_evaluate = my_data_generator_test(batch_size, width, height, num_ch, test_vis_data, test_nir_data, test_genuine_index_pair, test_genuine_count, test_imposter_index_pair, test_imposter_count, 'evaluate')\n",
    "\n",
    "test_gen_predict = my_data_generator_test(batch_size, width, height, num_ch, test_vis_data, test_nir_data, test_genuine_index_pair, test_genuine_count, test_imposter_index_pair, test_imposter_count, 'predict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1912d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net.evaluate(test_gen_evaluate,steps = int((test_genuine_count+ 15032)/batch_size),verbose=1)\n",
    "\n",
    "predictions_test=siamese_net.predict(test_gen_predict, steps=int((test_genuine_count + 15032)/batch_size),verbose=1)\n",
    "\n",
    "predictions_test_max=np.argmax(predictions_test,axis=1)\n",
    "np.shape(predictions_test_max)\n",
    "\n",
    "test_input_targets = np.concatenate((np.ones((test_genuine_count, )),  np.zeros((15032, ))), axis = 0)\n",
    "test_input_targets_categorical = to_categorical(test_input_targets, 2)\n",
    "print(test_input_targets.shape)\n",
    "print(test_input_targets_categorical.shape)\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, f1_score\n",
    "p= precision_score(test_input_targets, predictions_test_max)\n",
    "r= recall_score(test_input_targets, predictions_test_max)\n",
    "print(\"P=\", p)\n",
    "print(\"R=\", r)\n",
    "p= precision_score(test_input_targets, predictions_test_max, average='macro')\n",
    "r= recall_score(test_input_targets, predictions_test_max, average='macro')\n",
    "print(\"P_macro=\", p)\n",
    "print(\"R_macro=\", r)\n",
    "\n",
    "f1= f1_score(test_input_targets, predictions_test_max)\n",
    "print(\"f1=\", f1)\n",
    "\n",
    "f1= f1_score(test_input_targets, predictions_test_max, average='macro')\n",
    "print(\"f1_macro=\", f1)\n",
    "\n",
    "cc= confusion_matrix(test_input_targets, predictions_test_max)\n",
    "print(cc)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from numpy import *\n",
    "lw=2\n",
    "n_classes = 2\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_input_targets_categorical[:, i], predictions_test[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr \n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],label='ROC curve (area = {0:0.2f})' ''.format(roc_auc[\"macro\"]),color='r', linestyle='--', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\"),\n",
    "plt.show()\n",
    "\n",
    "name_test = 'wt_'+ name\n",
    "np.save (save_path +'pred_'+name_test+'.npy',predictions_test)\n",
    "np.save(save_path +'fpr_'+name_test+'.npy',all_fpr)\n",
    "np.save(save_path +'tpr_'+ name_test+'.npy',mean_tpr)\n",
    "plt.savefig(save_path +'ROC_'+name_test+'.png')\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004205fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196564/196564 [==============================] - 3523s 18ms/step - loss: 0.3417 - accuracy: 0.9666\n",
      "     1/196564 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_predict_batch_end` time: 0.0156s). Check your callbacks.\n",
      "196564/196564 [==============================] - 3531s 18ms/step\n",
      "(6290064,)\n",
      "(6290064, 2)\n"
     ]
    }
   ],
   "source": [
    "siamese_net.evaluate(test_gen_evaluate,steps = int((test_genuine_count+ test_imposter_count)/batch_size),verbose=1)\n",
    "\n",
    "predictions_test=siamese_net.predict(test_gen_predict, steps=int((test_genuine_count + test_imposter_count)/batch_size),verbose=1)\n",
    "\n",
    "predictions_test_max=np.argmax(predictions_test,axis=1)\n",
    "np.shape(predictions_test_max)\n",
    "\n",
    "test_input_targets = np.concatenate((np.ones((test_genuine_count, )),  np.zeros((test_imposter_count, ))), axis = 0)\n",
    "test_input_targets_categorical = to_categorical(test_input_targets, 2)\n",
    "print(test_input_targets.shape)\n",
    "print(test_input_targets_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2322335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6290048,)\n",
      "6290048\n"
     ]
    }
   ],
   "source": [
    "print(predictions_test_max.shape)\n",
    "print(predictions_test_max.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16674a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'C:/Sushree/Tensor_DCT/Results/'\n",
    "np.save (save_path + 'test_genuine_index_pair.npy', test_genuine_index_pair)\n",
    "np.save (save_path + 'test_genuine_folder_pair.npy', test_genuine_folder_pair)\n",
    "np.save (save_path + 'test_imposter_index_pair.npy', test_imposter_index_pair)\n",
    "np.save (save_path + 'test_imposter_folder_pair.npy', test_imposter_folder_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e178f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P= 0.0659070568905868\n",
      "R= 0.983652312599681\n",
      "P_macro= 0.532933249683208\n",
      "R_macro= 0.9751100606823107\n",
      "f1= 0.12353684948484582\n",
      "f1_macro= 0.5532584900633319\n",
      "[[6065213  209787]\n",
      " [    246   14802]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, f1_score\n",
    "p= precision_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max)\n",
    "r= recall_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max)\n",
    "print(\"P=\", p)\n",
    "print(\"R=\", r)\n",
    "p= precision_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max, average='macro')\n",
    "r= recall_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max, average='macro')\n",
    "print(\"P_macro=\", p)\n",
    "print(\"R_macro=\", r)\n",
    "\n",
    "f1= f1_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max)\n",
    "print(\"f1=\", f1)\n",
    "\n",
    "f1= f1_score(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max, average='macro')\n",
    "print(\"f1_macro=\", f1)\n",
    "\n",
    "cc= confusion_matrix(test_input_targets[0:predictions_test_max.shape[0]], predictions_test_max)\n",
    "print(cc)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "from numpy import *\n",
    "lw=2\n",
    "n_classes = 2\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(test_input_targets_categorical[0:predictions_test_max.shape[0], i], predictions_test[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr \n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],label='ROC curve (area = {0:0.2f})' ''.format(roc_auc[\"macro\"]),color='r', linestyle='--', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic curve')\n",
    "plt.legend(loc=\"lower right\"),\n",
    "plt.show()\n",
    "\n",
    "name_test = 'wt_'+ name +'_tstAll'\n",
    "np.save (save_path +'pred_'+name_test+'.npy',predictions_test)\n",
    "np.save(save_path +'fpr_'+name_test+'.npy',all_fpr)\n",
    "np.save(save_path +'tpr_'+ name_test+'.npy',mean_tpr)\n",
    "plt.savefig(save_path +'ROC_'+name_test+'.png')\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae82db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
